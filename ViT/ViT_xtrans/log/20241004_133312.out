the modelViT(
  (patch_embedding): Linear(in_features=192, out_features=256, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (transformer): AttentionLayers(
    (layers): ModuleList(
      (0): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): Attention(
          (to_q): Linear(in_features=256, out_features=512, bias=False)
          (to_k): Linear(in_features=256, out_features=512, bias=False)
          (to_v): Linear(in_features=256, out_features=512, bias=False)
          (attend): Attend(
            (attn_dropout): Dropout(p=0.1, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=256, bias=False)
        )
        (2): Residual()
      )
      (1): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): MyGELU()
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
        (2): Residual()
      )
      (2): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): Attention(
          (to_q): Linear(in_features=256, out_features=512, bias=False)
          (to_k): Linear(in_features=256, out_features=512, bias=False)
          (to_v): Linear(in_features=256, out_features=512, bias=False)
          (attend): Attend(
            (attn_dropout): Dropout(p=0.1, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=256, bias=False)
        )
        (2): Residual()
      )
      (3): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): MyGELU()
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
        (2): Residual()
      )
      (4): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): Attention(
          (to_q): Linear(in_features=256, out_features=512, bias=False)
          (to_k): Linear(in_features=256, out_features=512, bias=False)
          (to_v): Linear(in_features=256, out_features=512, bias=False)
          (attend): Attend(
            (attn_dropout): Dropout(p=0.1, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=256, bias=False)
        )
        (2): Residual()
      )
      (5): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): MyGELU()
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
        (2): Residual()
      )
      (6): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): Attention(
          (to_q): Linear(in_features=256, out_features=512, bias=False)
          (to_k): Linear(in_features=256, out_features=512, bias=False)
          (to_v): Linear(in_features=256, out_features=512, bias=False)
          (attend): Attend(
            (attn_dropout): Dropout(p=0.1, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=256, bias=False)
        )
        (2): Residual()
      )
      (7): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): MyGELU()
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
        (2): Residual()
      )
      (8): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): Attention(
          (to_q): Linear(in_features=256, out_features=512, bias=False)
          (to_k): Linear(in_features=256, out_features=512, bias=False)
          (to_v): Linear(in_features=256, out_features=512, bias=False)
          (attend): Attend(
            (attn_dropout): Dropout(p=0.1, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=256, bias=False)
        )
        (2): Residual()
      )
      (9): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): MyGELU()
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
        (2): Residual()
      )
      (10): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): Attention(
          (to_q): Linear(in_features=256, out_features=512, bias=False)
          (to_k): Linear(in_features=256, out_features=512, bias=False)
          (to_v): Linear(in_features=256, out_features=512, bias=False)
          (attend): Attend(
            (attn_dropout): Dropout(p=0.1, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=256, bias=False)
        )
        (2): Residual()
      )
      (11): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): MyGELU()
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
        (2): Residual()
      )
    )
    (adaptive_mlp): Identity()
    (final_norm): LayerNorm(
      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
    )
    (skip_combines): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
      (5): None
      (6): None
      (7): None
      (8): None
      (9): None
      (10): None
      (11): None
    )
  )
  (LN): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (to_cls_token): Identity()
  (fc): Linear(in_features=256, out_features=10, bias=True)
)
the transformerAttentionLayers(
  (layers): ModuleList(
    (0): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): Attention(
        (to_q): Linear(in_features=256, out_features=512, bias=False)
        (to_k): Linear(in_features=256, out_features=512, bias=False)
        (to_v): Linear(in_features=256, out_features=512, bias=False)
        (attend): Attend(
          (attn_dropout): Dropout(p=0.1, inplace=False)
        )
        (to_out): Linear(in_features=512, out_features=256, bias=False)
      )
      (2): Residual()
    )
    (1): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): FeedForward(
        (ff): Sequential(
          (0): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): MyGELU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=1024, out_features=256, bias=True)
        )
      )
      (2): Residual()
    )
    (2): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): Attention(
        (to_q): Linear(in_features=256, out_features=512, bias=False)
        (to_k): Linear(in_features=256, out_features=512, bias=False)
        (to_v): Linear(in_features=256, out_features=512, bias=False)
        (attend): Attend(
          (attn_dropout): Dropout(p=0.1, inplace=False)
        )
        (to_out): Linear(in_features=512, out_features=256, bias=False)
      )
      (2): Residual()
    )
    (3): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): FeedForward(
        (ff): Sequential(
          (0): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): MyGELU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=1024, out_features=256, bias=True)
        )
      )
      (2): Residual()
    )
    (4): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): Attention(
        (to_q): Linear(in_features=256, out_features=512, bias=False)
        (to_k): Linear(in_features=256, out_features=512, bias=False)
        (to_v): Linear(in_features=256, out_features=512, bias=False)
        (attend): Attend(
          (attn_dropout): Dropout(p=0.1, inplace=False)
        )
        (to_out): Linear(in_features=512, out_features=256, bias=False)
      )
      (2): Residual()
    )
    (5): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): FeedForward(
        (ff): Sequential(
          (0): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): MyGELU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=1024, out_features=256, bias=True)
        )
      )
      (2): Residual()
    )
    (6): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): Attention(
        (to_q): Linear(in_features=256, out_features=512, bias=False)
        (to_k): Linear(in_features=256, out_features=512, bias=False)
        (to_v): Linear(in_features=256, out_features=512, bias=False)
        (attend): Attend(
          (attn_dropout): Dropout(p=0.1, inplace=False)
        )
        (to_out): Linear(in_features=512, out_features=256, bias=False)
      )
      (2): Residual()
    )
    (7): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): FeedForward(
        (ff): Sequential(
          (0): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): MyGELU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=1024, out_features=256, bias=True)
        )
      )
      (2): Residual()
    )
    (8): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): Attention(
        (to_q): Linear(in_features=256, out_features=512, bias=False)
        (to_k): Linear(in_features=256, out_features=512, bias=False)
        (to_v): Linear(in_features=256, out_features=512, bias=False)
        (attend): Attend(
          (attn_dropout): Dropout(p=0.1, inplace=False)
        )
        (to_out): Linear(in_features=512, out_features=256, bias=False)
      )
      (2): Residual()
    )
    (9): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): FeedForward(
        (ff): Sequential(
          (0): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): MyGELU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=1024, out_features=256, bias=True)
        )
      )
      (2): Residual()
    )
    (10): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): Attention(
        (to_q): Linear(in_features=256, out_features=512, bias=False)
        (to_k): Linear(in_features=256, out_features=512, bias=False)
        (to_v): Linear(in_features=256, out_features=512, bias=False)
        (attend): Attend(
          (attn_dropout): Dropout(p=0.1, inplace=False)
        )
        (to_out): Linear(in_features=512, out_features=256, bias=False)
      )
      (2): Residual()
    )
    (11): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): FeedForward(
        (ff): Sequential(
          (0): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): MyGELU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=1024, out_features=256, bias=True)
        )
      )
      (2): Residual()
    )
  )
  (adaptive_mlp): Identity()
  (final_norm): LayerNorm(
    (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
  )
  (skip_combines): ModuleList(
    (0): None
    (1): None
    (2): None
    (3): None
    (4): None
    (5): None
    (6): None
    (7): None
    (8): None
    (9): None
    (10): None
    (11): None
  )
)
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0.0005
)
Epoch 0: train loss 1.966930245866581, val loss 1.7118428111076356, val accuracy 37.17, train acc 26.232
Epoch 1: train loss 1.6895845502006763, val loss 1.5439670979976654, val accuracy 44.06, train acc 37.974
Epoch 2: train loss 1.6041959840424207, val loss 1.4898440003395081, val accuracy 47.19, train acc 41.44
Epoch 3: train loss 1.5500390626946274, val loss 1.4661370038986206, val accuracy 46.92, train acc 43.598
Epoch 4: train loss 1.5178267621264165, val loss 1.417997121810913, val accuracy 48.83, train acc 44.668
Epoch 5: train loss 1.4886200640882765, val loss 1.3668517887592315, val accuracy 51.26, train acc 46.104
Epoch 6: train loss 1.4547223752858687, val loss 1.35774764418602, val accuracy 51.31, train acc 46.946
Epoch 7: train loss 1.438116015220175, val loss 1.371881228685379, val accuracy 50.86, train acc 47.784
Epoch 8: train loss 1.4171573106123476, val loss 1.3351106107234956, val accuracy 51.55, train acc 48.608
Epoch 9: train loss 1.391795539734315, val loss 1.3652260422706604, val accuracy 51.8, train acc 49.852
Epoch 10: train loss 1.3840454585698185, val loss 1.3083826541900634, val accuracy 52.82, train acc 49.906
Epoch 11: train loss 1.3704465870954552, val loss 1.277478528022766, val accuracy 53.9, train acc 50.404
Epoch 12: train loss 1.3541581040742445, val loss 1.2540839076042176, val accuracy 55.0, train acc 51.164
Epoch 13: train loss 1.3304374984332494, val loss 1.238373690843582, val accuracy 55.77, train acc 52.058
Epoch 14: train loss 1.3250681715352195, val loss 1.2379405796527863, val accuracy 55.36, train acc 52.346
Epoch 15: train loss 1.3094810521116063, val loss 1.240140175819397, val accuracy 55.1, train acc 52.424
Epoch 16: train loss 1.29628231574078, val loss 1.2233877778053284, val accuracy 55.93, train acc 53.144
Epoch 17: train loss 1.2865402765420018, val loss 1.2258273839950562, val accuracy 56.06, train acc 53.744
Epoch 18: train loss 1.2735533568323876, val loss 1.1871611595153808, val accuracy 57.14, train acc 54.31
Epoch 19: train loss 1.2590769505014225, val loss 1.193991631269455, val accuracy 56.88, train acc 54.54
Epoch 20: train loss 1.249642243798898, val loss 1.2242496609687805, val accuracy 56.41, train acc 54.81
Epoch 21: train loss 1.2369591155830695, val loss 1.1939624011516572, val accuracy 57.42, train acc 55.452
Epoch 22: train loss 1.2343404055858145, val loss 1.1975550711154939, val accuracy 56.76, train acc 55.568
Epoch 23: train loss 1.2132975608110428, val loss 1.1349644601345061, val accuracy 59.7, train acc 56.512
Epoch 24: train loss 1.210779919308059, val loss 1.143725037574768, val accuracy 59.18, train acc 56.562
Epoch 25: train loss 1.2083108923873123, val loss 1.166979306936264, val accuracy 57.68, train acc 56.612
Epoch 26: train loss 1.1885491335878566, val loss 1.157950186729431, val accuracy 58.33, train acc 57.242
Epoch 27: train loss 1.183327390831344, val loss 1.1122853815555573, val accuracy 59.54, train acc 57.384
Epoch 28: train loss 1.1789325159423205, val loss 1.149563765525818, val accuracy 58.99, train acc 57.628
Epoch 29: train loss 1.1718440280885112, val loss 1.1618167221546174, val accuracy 59.15, train acc 57.868
Epoch 30: train loss 1.1707679355631069, val loss 1.1034305155277253, val accuracy 60.85, train acc 57.95
Epoch 31: train loss 1.155872333718806, val loss 1.0916072010993958, val accuracy 60.69, train acc 58.502
Epoch 32: train loss 1.1531763323107544, val loss 1.1198853313922883, val accuracy 59.84, train acc 58.678
Epoch 33: train loss 1.1381791802693386, val loss 1.0801976501941681, val accuracy 61.54, train acc 59.26
Epoch 34: train loss 1.1344930642113393, val loss 1.1130742728710175, val accuracy 60.47, train acc 59.312
Epoch 35: train loss 1.1343125740484314, val loss 1.0811195433139802, val accuracy 61.49, train acc 59.488
Epoch 36: train loss 1.1301213451186005, val loss 1.0683597087860108, val accuracy 61.89, train acc 59.632
Epoch 37: train loss 1.1211465563701124, val loss 1.1245883882045746, val accuracy 60.24, train acc 59.732
Epoch 38: train loss 1.1164403746322709, val loss 1.0518174797296524, val accuracy 62.17, train acc 60.158
Epoch 39: train loss 1.1102480489988715, val loss 1.0502828240394593, val accuracy 62.7, train acc 60.272
Epoch 40: train loss 1.1060990277601748, val loss 1.0140387803316115, val accuracy 64.33, train acc 60.32
Epoch 41: train loss 1.1016954293056411, val loss 1.0326368361711502, val accuracy 63.54, train acc 60.69
Epoch 42: train loss 1.0961672800536058, val loss 1.042930793762207, val accuracy 62.45, train acc 60.974
Epoch 43: train loss 1.0932068657510134, val loss 1.041931015253067, val accuracy 62.99, train acc 60.88
Epoch 44: train loss 1.0879483302028812, val loss 1.0424512892961502, val accuracy 62.53, train acc 61.098
Epoch 45: train loss 1.0823267144816262, val loss 1.0472612380981445, val accuracy 62.32, train acc 61.382
Epoch 46: train loss 1.0783778222239748, val loss 1.047196701169014, val accuracy 62.54, train acc 61.518
Epoch 47: train loss 1.0690246707930857, val loss 1.0342828810214997, val accuracy 63.0, train acc 61.858
Epoch 48: train loss 1.0728045510394233, val loss 1.04790900349617, val accuracy 62.79, train acc 61.532
Epoch 49: train loss 1.0618232719752254, val loss 0.9984000951051712, val accuracy 63.92, train acc 62.004
