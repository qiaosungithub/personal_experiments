the modelViT(
  (patch_embedding): Linear(in_features=192, out_features=256, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (transformer): AttentionLayers(
    (layers): ModuleList(
      (0): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): Attention(
          (to_q): Linear(in_features=256, out_features=512, bias=False)
          (to_k): Linear(in_features=256, out_features=512, bias=False)
          (to_v): Linear(in_features=256, out_features=512, bias=False)
          (attend): Attend(
            (attn_dropout): Dropout(p=0.1, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=256, bias=False)
        )
        (2): Residual()
      )
      (1): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): MyGELU()
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
        (2): Residual()
      )
      (2): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): Attention(
          (to_q): Linear(in_features=256, out_features=512, bias=False)
          (to_k): Linear(in_features=256, out_features=512, bias=False)
          (to_v): Linear(in_features=256, out_features=512, bias=False)
          (attend): Attend(
            (attn_dropout): Dropout(p=0.1, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=256, bias=False)
        )
        (2): Residual()
      )
      (3): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): MyGELU()
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
        (2): Residual()
      )
      (4): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): Attention(
          (to_q): Linear(in_features=256, out_features=512, bias=False)
          (to_k): Linear(in_features=256, out_features=512, bias=False)
          (to_v): Linear(in_features=256, out_features=512, bias=False)
          (attend): Attend(
            (attn_dropout): Dropout(p=0.1, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=256, bias=False)
        )
        (2): Residual()
      )
      (5): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): MyGELU()
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
        (2): Residual()
      )
      (6): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): Attention(
          (to_q): Linear(in_features=256, out_features=512, bias=False)
          (to_k): Linear(in_features=256, out_features=512, bias=False)
          (to_v): Linear(in_features=256, out_features=512, bias=False)
          (attend): Attend(
            (attn_dropout): Dropout(p=0.1, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=256, bias=False)
        )
        (2): Residual()
      )
      (7): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): MyGELU()
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
        (2): Residual()
      )
      (8): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): Attention(
          (to_q): Linear(in_features=256, out_features=512, bias=False)
          (to_k): Linear(in_features=256, out_features=512, bias=False)
          (to_v): Linear(in_features=256, out_features=512, bias=False)
          (attend): Attend(
            (attn_dropout): Dropout(p=0.1, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=256, bias=False)
        )
        (2): Residual()
      )
      (9): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): MyGELU()
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
        (2): Residual()
      )
      (10): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): Attention(
          (to_q): Linear(in_features=256, out_features=512, bias=False)
          (to_k): Linear(in_features=256, out_features=512, bias=False)
          (to_v): Linear(in_features=256, out_features=512, bias=False)
          (attend): Attend(
            (attn_dropout): Dropout(p=0.1, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=256, bias=False)
        )
        (2): Residual()
      )
      (11): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          )
          (1): None
          (2): None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): MyGELU()
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
        (2): Residual()
      )
    )
    (adaptive_mlp): Identity()
    (final_norm): LayerNorm(
      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
    )
    (skip_combines): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
      (5): None
      (6): None
      (7): None
      (8): None
      (9): None
      (10): None
      (11): None
    )
  )
  (LN): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (to_cls_token): Identity()
  (fc): Linear(in_features=256, out_features=10, bias=True)
)
the transformerAttentionLayers(
  (layers): ModuleList(
    (0): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): Attention(
        (to_q): Linear(in_features=256, out_features=512, bias=False)
        (to_k): Linear(in_features=256, out_features=512, bias=False)
        (to_v): Linear(in_features=256, out_features=512, bias=False)
        (attend): Attend(
          (attn_dropout): Dropout(p=0.1, inplace=False)
        )
        (to_out): Linear(in_features=512, out_features=256, bias=False)
      )
      (2): Residual()
    )
    (1): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): FeedForward(
        (ff): Sequential(
          (0): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): MyGELU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=1024, out_features=256, bias=True)
        )
      )
      (2): Residual()
    )
    (2): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): Attention(
        (to_q): Linear(in_features=256, out_features=512, bias=False)
        (to_k): Linear(in_features=256, out_features=512, bias=False)
        (to_v): Linear(in_features=256, out_features=512, bias=False)
        (attend): Attend(
          (attn_dropout): Dropout(p=0.1, inplace=False)
        )
        (to_out): Linear(in_features=512, out_features=256, bias=False)
      )
      (2): Residual()
    )
    (3): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): FeedForward(
        (ff): Sequential(
          (0): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): MyGELU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=1024, out_features=256, bias=True)
        )
      )
      (2): Residual()
    )
    (4): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): Attention(
        (to_q): Linear(in_features=256, out_features=512, bias=False)
        (to_k): Linear(in_features=256, out_features=512, bias=False)
        (to_v): Linear(in_features=256, out_features=512, bias=False)
        (attend): Attend(
          (attn_dropout): Dropout(p=0.1, inplace=False)
        )
        (to_out): Linear(in_features=512, out_features=256, bias=False)
      )
      (2): Residual()
    )
    (5): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): FeedForward(
        (ff): Sequential(
          (0): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): MyGELU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=1024, out_features=256, bias=True)
        )
      )
      (2): Residual()
    )
    (6): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): Attention(
        (to_q): Linear(in_features=256, out_features=512, bias=False)
        (to_k): Linear(in_features=256, out_features=512, bias=False)
        (to_v): Linear(in_features=256, out_features=512, bias=False)
        (attend): Attend(
          (attn_dropout): Dropout(p=0.1, inplace=False)
        )
        (to_out): Linear(in_features=512, out_features=256, bias=False)
      )
      (2): Residual()
    )
    (7): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): FeedForward(
        (ff): Sequential(
          (0): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): MyGELU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=1024, out_features=256, bias=True)
        )
      )
      (2): Residual()
    )
    (8): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): Attention(
        (to_q): Linear(in_features=256, out_features=512, bias=False)
        (to_k): Linear(in_features=256, out_features=512, bias=False)
        (to_v): Linear(in_features=256, out_features=512, bias=False)
        (attend): Attend(
          (attn_dropout): Dropout(p=0.1, inplace=False)
        )
        (to_out): Linear(in_features=512, out_features=256, bias=False)
      )
      (2): Residual()
    )
    (9): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): FeedForward(
        (ff): Sequential(
          (0): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): MyGELU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=1024, out_features=256, bias=True)
        )
      )
      (2): Residual()
    )
    (10): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): Attention(
        (to_q): Linear(in_features=256, out_features=512, bias=False)
        (to_k): Linear(in_features=256, out_features=512, bias=False)
        (to_v): Linear(in_features=256, out_features=512, bias=False)
        (attend): Attend(
          (attn_dropout): Dropout(p=0.1, inplace=False)
        )
        (to_out): Linear(in_features=512, out_features=256, bias=False)
      )
      (2): Residual()
    )
    (11): ModuleList(
      (0): ModuleList(
        (0): LayerNorm(
          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
        )
        (1): None
        (2): None
      )
      (1): FeedForward(
        (ff): Sequential(
          (0): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): MyGELU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=1024, out_features=256, bias=True)
        )
      )
      (2): Residual()
    )
  )
  (adaptive_mlp): Identity()
  (final_norm): LayerNorm(
    (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
  )
  (skip_combines): ModuleList(
    (0): None
    (1): None
    (2): None
    (3): None
    (4): None
    (5): None
    (6): None
    (7): None
    (8): None
    (9): None
    (10): None
    (11): None
  )
)
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0.0005
)
Epoch 0: train loss 1.0638830202574632, val loss 1.079873153567314, val accuracy 61.26, train acc 61.958
Epoch 1: train loss 1.0558927703876884, val loss 1.029577910900116, val accuracy 63.72, train acc 62.45
Epoch 2: train loss 1.0529755953015114, val loss 1.0361034512519836, val accuracy 63.36, train acc 62.338
Epoch 3: train loss 1.0491333178111486, val loss 0.9904558956623077, val accuracy 64.49, train acc 62.522
Epoch 4: train loss 1.0454257480343994, val loss 1.038786417245865, val accuracy 63.36, train acc 62.798
Epoch 5: train loss 1.0421217333297341, val loss 1.0210485190153122, val accuracy 63.49, train acc 63.028
Epoch 6: train loss 1.044683934474478, val loss 0.9970892697572709, val accuracy 63.85, train acc 62.538
Epoch 7: train loss 1.0336320156953773, val loss 0.98452108502388, val accuracy 64.56, train acc 62.806
Epoch 8: train loss 1.0321530724058345, val loss 0.9600220590829849, val accuracy 65.4, train acc 63.024
Epoch 9: train loss 1.0303129502096955, val loss 0.9955902904272079, val accuracy 64.39, train acc 63.19
Epoch 10: train loss 1.0248234223346322, val loss 0.9948921591043473, val accuracy 64.72, train acc 63.37
Epoch 11: train loss 1.0204342542862406, val loss 0.9725744634866714, val accuracy 65.1, train acc 63.64
Epoch 12: train loss 1.021677063131819, val loss 0.9562772244215012, val accuracy 65.84, train acc 63.414
Epoch 13: train loss 1.018107845466964, val loss 0.9505616039037704, val accuracy 65.92, train acc 63.694
Epoch 14: train loss 1.0092901146533537, val loss 1.0186859101057053, val accuracy 63.25, train acc 64.064
Epoch 15: train loss 1.0118156333967132, val loss 1.033725106716156, val accuracy 62.68, train acc 63.89
Epoch 16: train loss 1.0066139442580087, val loss 0.9954469233751297, val accuracy 64.12, train acc 64.242
Epoch 17: train loss 1.0005600975484263, val loss 0.9840713113546371, val accuracy 64.45, train acc 64.494
Epoch 18: train loss 1.0006565685783113, val loss 0.9765820235013962, val accuracy 65.39, train acc 64.336
Epoch 19: train loss 1.002226392833554, val loss 0.9352709770202636, val accuracy 66.44, train acc 64.018
Epoch 20: train loss 0.9926363001672589, val loss 0.9564878433942795, val accuracy 66.13, train acc 64.444
Epoch 21: train loss 0.9932148268028181, val loss 1.038650307059288, val accuracy 63.37, train acc 64.798
Epoch 22: train loss 0.9926475614917522, val loss 0.9406078845262528, val accuracy 66.11, train acc 64.536
Epoch 23: train loss 0.9921866734417117, val loss 0.9580666154623032, val accuracy 66.1, train acc 64.64
Epoch 24: train loss 0.9905095812009306, val loss 0.9710371613502502, val accuracy 65.01, train acc 64.632
Epoch 25: train loss 0.98973932588587, val loss 0.9950110942125321, val accuracy 64.59, train acc 64.702
Epoch 26: train loss 0.9823851232626, val loss 0.9699319928884507, val accuracy 65.66, train acc 65.002
Epoch 27: train loss 0.9773532733017084, val loss 0.9421231001615524, val accuracy 66.4, train acc 65.082
Epoch 28: train loss 0.975875970052213, val loss 0.9729517608880996, val accuracy 65.41, train acc 65.164
Epoch 29: train loss 0.9823842301052443, val loss 1.0109850406646728, val accuracy 64.44, train acc 65.006
Epoch 30: train loss 0.9756740325567673, val loss 0.9626878589391709, val accuracy 65.87, train acc 65.328
Epoch 31: train loss 0.9708698382791208, val loss 0.9389294713735581, val accuracy 66.14, train acc 65.33
Epoch 32: train loss 0.9702927473248267, val loss 0.9572450548410416, val accuracy 66.33, train acc 65.304
Epoch 33: train loss 0.9659920343939139, val loss 0.9375452697277069, val accuracy 66.43, train acc 65.504
Epoch 34: train loss 0.9642786848910001, val loss 0.972611665725708, val accuracy 65.22, train acc 65.662
Epoch 35: train loss 0.966451076220493, val loss 0.9204259306192398, val accuracy 67.29, train acc 65.316
Epoch 36: train loss 0.9595086720525002, val loss 0.9207537978887558, val accuracy 67.32, train acc 65.722
Epoch 37: train loss 0.9628004796650945, val loss 0.9743075489997863, val accuracy 65.68, train acc 65.87
Epoch 38: train loss 0.9610305449792317, val loss 0.9337925791740418, val accuracy 67.06, train acc 65.728
Epoch 39: train loss 0.9473837957698472, val loss 0.9429512798786164, val accuracy 66.98, train acc 66.166
Epoch 40: train loss 0.9540166952172104, val loss 0.9429041773080826, val accuracy 66.47, train acc 66.014
Epoch 41: train loss 0.9525265505119246, val loss 0.9286228567361832, val accuracy 66.94, train acc 65.964
Epoch 42: train loss 0.9483006131284091, val loss 0.9564226508140564, val accuracy 66.42, train acc 66.622
Epoch 43: train loss 0.945020255385613, val loss 0.9357128322124482, val accuracy 66.75, train acc 66.216
Epoch 44: train loss 0.9440154174760896, val loss 0.903874060511589, val accuracy 68.27, train acc 66.364
Epoch 45: train loss 0.9431894044486844, val loss 0.9304310083389282, val accuracy 67.32, train acc 66.332
Epoch 46: train loss 0.9492094860393174, val loss 0.9180210053920745, val accuracy 67.01, train acc 66.308
Epoch 47: train loss 0.9410187009037757, val loss 0.922388881444931, val accuracy 67.19, train acc 66.428
Epoch 48: train loss 0.9413785529988152, val loss 0.9192634761333466, val accuracy 67.14, train acc 66.538
Epoch 49: train loss 0.9336733212884591, val loss 0.9377864629030228, val accuracy 67.03, train acc 66.814
